%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{article}
\usepackage{fixme}
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\fxsetup{status=draft} % <====== add this line
\newtheorem{definition}{Definition}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2023}
%\acmYear{2023}


%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[GECCO '23]{Genetic Evolutionary Computation}{2023}{Lisbon, Portugal}
\acmBooktitle{GECCO '23: xx, xx, xx}
%\acmPrice{15.00}
\acmISBN{xxxxx}

\acmDOI{xxxx}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{VNS with GA-based parameter tuning for solving the $k$-domination problem}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Milan Predojević}
 
\email{milan.predojevic@pmf.unibl.org}
\orcid{xxxx-xxxx-xxxx}
\author{M.P.}
\authornote{Both authors contributed equally to this research.}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Faculty of Sciences and Mathematics, University of Banja Luka}
  \streetaddress{Mladen Stojanovi\'c 2}
  \city{Banja Luka}
  \state{Serb Republic}
  \country{Bosnia and Herzegovina}
  \postcode{78000}
}

\author{Aleksandar Kartelj}
\author{A.K.}
\affiliation{%
  \institution{Faculty of Mathematics, Univeristy of Belgrade}
  \streetaddress{--}
  \city{Belgrade}
  \country{Serbia}}
\authornote{Both authors contributed equally to this research.}
\email{ kartelj@math.rs}

\author{Marko Djukanović}
 
\email{marko.djukanovic@pmf.unibl.org}
\orcid{xxxx-xxxx-xxxx}
\author{M.D.}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
\affiliation{%
	\institution{Faculty of Sciences and Mathematics, University of Banja Luka}
	\streetaddress{Mladen Stojanovi\'c 2}
	\city{Banja Luka}
	\state{Serb Republic}
	\country{Bosnia and Herzegovina}
	\postcode{78000}
}
 
 
 
 

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Predojevic et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In this paper we are concerned with solving a generalized version of the well-known minimum dominating set problem, the so-called $k$-domination problem, $k \in \mathbb{N}$. This problem is about finding a minimal cardinality subset $D$ of nodes of a graph  $G=(V, E) $ such that every $v \in V$ belongs to $D$ or has at least $k$ neighbours from $D$. The $k$-domination problem has applications in distributed systems, biological networks etc. We propose a variable neighbourhood search (VNS) meta-heuristic for solving the $k$-domination problem. The VNS is equipped with an efficient fitness function that allows it to consider both feasible and infeasible solutions, while appropriately penalising infeasible solutions. The control parameters of the VNS are tuned using a genetic algorithm. The method is compared with the best known heuristic approaches from the literature: a beam search and the two greedy approaches. Experimental evaluations are performed on a real-world benchmark set whose instances represent the road networks of different cities. The VNS provided new state-of-the-art results for most of the considered instances with $k \in {1, 2, 4}$.
  
\end{abstract}
  
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{variable neighborhood search, graph domination, genetic algorithm,  parameter tuning}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
A graph $G=(V,E)$ is an abstract mathematical structure 
where $V$ represents a set of elements called vertices and   a set of pairs $e = uv  \in E \subseteq V \times V$ called edges of $G$. In the context of this work, we deal with  simple undirected graphs which have no loops and where edges have no directions, that is $e = \{u,v\}  = uv = vu \in E$.   Graphs serve as models of many real--world problems describing relations between objects in biology, physics, social networks,  etc.~\cite{mashaghi2004investigation,pirzada2007applications,shah2019characterizing,doi:10.1137/S0895480100375831}. One of the most prominent classes of problems that have been studied for decades from theoretical, computational and practical aspects are dominating problems on graphs~ \cite{haynes2013fundamentals}. The basic problem of this class is the \textit{domination problem}. The subset $D \subset V$ is called a domination set iff each vertex $v\in V$ belongs to $D$ or there is at least one vertex $w\in D$ such that $uw\in E$. Finding the smallest possible dominating set $D$ of graph $G$ w.r.t.\ its cardinalty defines \emph{the minimum dominating set problem} (MDSP)~\cite{grandoni2006note}. This problem has many applications, for example analysing biological networks~\cite{nacher2016minimum}, document summarization~\cite{shen2010multi}, graph mining~\cite{chalupa2018order},  etc. From algorithmic point of view, this problem is NP--hard. It is solved by many exact approaches such as branch-and-reduce algorithms~\cite{van2011exact}, an approach that uses the fundamental cut-sets of graph~\cite{karci2020new}, etc. On the other hand, heuristic approaches are dominant in the literature, for example a genetic algorithm~\cite{hedar2010hybrid}, simulated annealing~\cite{hedar2012simulated}, ant-colony optimization~\cite{ho2006enhanced} are just some among others. There are many generalizations of MDSP proposed in the literature that arise from practice: the minimum weight dominating set problem~\cite{romania2010ant}, the minimum total dominating problem~\cite{yuan2019novel}, the minimum connected dominating set problem~\cite{butenko2004new}, etc.




In the course of this work, we study the \emph{minimum k-domination problem} (MkDP)~\cite{corcoran2021heuristics} for a fixed $k \in \mathbb{N}$.  Note that the definition for this problem is ambiguous in the literature. One variant asks for finding a minimum cardinality vertex set $ D$ such that every vertex of $G$ is within distance $k$ from some vertex in $D$~\cite{chang1983k}. 
This paper considers the following definition.  A $k$-dominating set  $D$ of a graph $G$ is a subset such that every vertex does not belong to $D$ is adjacent with at least $k$ vertices in  $D$~\cite{lan2013algorithmic}. A minimum $k$--dominating set represents the optimal solution of MkDP.  NP-completeness of the k-domination problem is proven on split graphs, where the problem is studied from an algorithmic point of view, see details in the aforementioned citation. A Beam search approach to solve the MkDP is proposed by Corcoran and Gagarin   in~\cite{corcoran2021heuristics}, so-far leading heuristic approach on small-to-middle sized real--world instances. A few greedy approaches is proposed by Couture et al.~\cite{couture2006incremental}, Gagarin and Corcoran~\cite{gagarin2018multiple}, and  Gagarin et al.~\cite{gagarin2013randomized} who proposed a randomized greedy algorithm. However, any of the proposed methods in the literature do not scale well with increase in the instance size.   
Applications of MkDP can be found in  distributed systems  ~\cite{wang2013minimising} where  a $k$-dominating set in a distributed system is a set of processors such that each processor outside the set has at least $k$ neighbors in the set.

In this work we propose another meta-heuristic approach to solve the MkDP, a variable neighborhood search (VNS). The main algorithm components of the approach are:
\begin{itemize}
	\item A carefully designed fitness function that evaluates solutions  including also unfeasible ones; it takes into consideration two scores ($i$) the size of dominating set; ($ii$) a penalized measure that evaluated how far is the considered set from being a $k$--dominating. 
	\item A shaking procedure that systematically destructs feasible solutions and ensures escaping the algorithm from a local optima.
	\item An efficient local search mechanism that try to fix unfeasible solutions into feasible ones. It is equipped witch a fast partial evaluation of the fitness function in a constant time.
 
\end{itemize}
Contributions of the paper are as follow.
\begin{itemize}
	\item The parameters of the proposed VNS are tuned by two kinds of tuning tools: the Grid Search algorithm~\cite{ranjan2019k} and an evolutionary-based tuning tool, an open-source Python library~\texttt{PyGAD}~\cite{gad2021pygad}.  
	\item The two settings of VNS, obtained by the above-mentioned tuning tools, are able to achieve new state-of-the-art results concerning the real-world instances; they are compared to the two greedy algorithms and the BS approach from the literature. 
	\item VNS is able to quickly produce solutions of a reasonable quality which is not the case of the two state-of-the-art incremental approaches from the literature.
	\item Concerning the (five) large-sized instances, where BS could not finish within the proposed time limit, our VNS delivers in all cases a feasible solution quickly thus showing a better anytime behaviour than the competitor approaches. (TODO: provjeriti ovo...)
	\item The proposed \textsc{Vns} scales better with the instance size increase than the other literature approaches from literature. \fxnote{Provjeriti ovo...}
\end{itemize}

\section{Notation and problem definition }
    

    Let $G=(V,E)$ be a simple undirected graph and $k \in \mathbf{N}$. For $v\in V$, by $N(v)$ we define a set of all neighboring vertices of $v$ in graph  $G$, that is $\{w \mid uw \in E\}$. A set $D \subseteq V$ is called a $k$--dominating iff for each $v\in V \setminus D$ it holds $|N(v) \cap D| \geq k$.  The MkDP is an optimization problem that asks for finding among all $k$--dominating sets of $G$ that of a minimum cardinalty. 
    
    
    This problem can be described by using a level of coverage of neighborhoods of vertex $v$ w.r.t.\ $D$
    \begin{equation}
    	C(D, v) = \min(k, |N(v) \cap D|)
    \end{equation}
and  formulated as the following optimization problem 
\begin{align}
    \arg \max_{D \subset V } \sum_{v \in V\setminus D} C(D,v) \\
    s.t. \forall v \in V \setminus D, |N(v) \cap D| \geq k.
\end{align}
    
\emph{Search space}.    Concerning the search space of the MkDP, we relax it by the needs of our VNS by not only searching for feasible solutions but also considering unfeasible ones which will be penalized, see Section~\ref{sec:vns}.  In other words, any subset of the set of vertices $V$ is a candidate solution in our VNS procedure. Thus, the size of search space is exponential, i.e. $2^{|V|}$.Further, a solution will be encoded as a set structure (of indices of vectors). 
   
   
\section{The proposed algorithm}\label{sec:vns}

In this section we first give an overview of the variable neighborhood search (VNS) scheme. Then, the main details on the VNS to solve MkDP are provided: the fitness evaluation, shaking procedure and local search.
 
  \subsection{Variable neighborhood search}
 Variable neighborhood search is an effective meta-heuristic approach proposed by Mladenović and Hansen~\cite{mladenovic1997variable}. The basic idea of the approach consists of systematically exchanging the neighborhoods of incumbent solution in order to escape from getting stack the search in a  local optimum. It is a single-point search metaheuristic, thus trying to improve always one solution per each iteration. VNS had been proved as one of the most powerful meta-heuristic, delivering state-of-the-art results on a wide range of problems, such as scheduling problems~\cite{fleszar2004solving}, vehicle routing problems~\cite{rezgui2019application}, median problems~\cite{herran2019variable}, etc.  
  
  The pseudocode of the basic VNS scheme is given in Algorithm~\ref{alg:vns}.
  
     \begin{algorithm}[!t] 
  	\caption{VNS scheme}\label{alg:vns}
  	\begin{algorithmic}[1]
  		\STATE \textbf{Input:} initial solution $s_{best}$ , neighborhoods  $\mathcal{N}_{d_{\min}},\ldots, \mathcal{N}_{d_{\max}}$ 
  		\STATE \textbf{Output:} (improved) solution $s_{best}$
  		\STATE $d \gets  d_{\min}$
  		\WHILE{!(\emph{TerminationCriteriaMet()})}
  		\STATE  $s' \gets$  $\texttt{Shaking}(\mathcal{N}_d(s_{best}))$ \hspace{0.3cm}//\,shaking phase
  		\STATE $s' \gets  \texttt{LocalSearch}(s {'})$
  		\IF{$fitness(s') < fitness(s_{best})$}
  		\STATE $s _{best}\gets s'$
  		\STATE $d \gets d_{min}$
  		\ELSE 
  		\STATE $d \gets d+1$ \hspace{0.3cm}//\, use next (VNS) neighborhood
  		\IF{$d > d_{max}$}
  		\STATE $d\gets d_{min}$
  		\ENDIF
  		\ENDIF
  		\ENDWHILE
  		\STATE \textbf{return} $s_{best}$
  	\end{algorithmic}
  \end{algorithm}

 VNS takes for input a solution $s_{best}$ obtained by a greedy or some randomized procedure, $d_{min}, d_{max} \in \mathbf{N}$, 
  and a set of neighborhood structures $\mathcal{N}_{d_{min}}, \ldots, \mathcal{N}_{d_{max}}$.  
  Initially, the starting neighbor $k$ is set to the smallest one, i.e. $k_{min}$. Further, the algorithm steps in the main loop (lines 4--16) until one of the termination criteria has met. At each iteration, the following steps are executed: %(usually time limit exceeded or number of iterations has reach
  
  \begin{itemize}
  	\item \texttt{Shaking}: $d$-th neighbor of incumbent solution $s_{best}$ is considered, and a random solution $s'$ is picked from there;
  	\item  \texttt{LocalSearch}: solution $s'$ is possibly improved by a local search procedure;
  	\item If a possibly improved solution $s'$ is better than $s_{best}$, it is declared for a new incumbent, this $s_{best} = s'$, and $d$ is set back to $d_{min}$; otherwise $d$ is incrementally increased;
  	\item  If $d> d_{max}$, $d$ is again set back to $d_{min}$.
   \end{itemize}
    

   \subsection{Fitness function}
       In order to evaluate solution $s$, the following nonlinear \emph{fitness} function is used 
       \begin{align}\label{eq:fitness}
          \emph{fitness}(s) = ( 1+ penalty \cdot |s|) \times ( 1 + violate(s))
       \end{align}
       where 
       \begin{align}
       	   violate(s) = \sum_{v \in V \setminus s}   k - C(s, v)  
       \end{align}
   Note that this function operates on both, feasible and unfeasible solutions. 
       Motivation for integrating this function  in the search is threefold:
       \begin{itemize}
       	\item First, any feasible solution of size $|s|$ is preferable over any unfeasible solution of the same cardinalty. 
       	\item Between two unfeasible solutions of the same cardinalty, the search prefers that whose \emph{violate} score is smaller, thus the solution is occasionally easier to be reshaped into a feasible one.  
       	\item Between two feasible solutions, better is the one with a smaller cardinalty. 
       \end{itemize}
     Note that a solution is structured as a set of all vertices that belong to it. 
   
   \subsection{Shaking}
    As we already mentioned, the purpose of shaking is to escape the search from stacking into   local optima. In case of MkDP, the shaking is realized as follows: \fxnote{TODO: Milan} 
   \subsection{Local search}
  The purpose of local search (LS) is improving a solution by applying small changes on solution structure. Therefore, defining  ``small changes'', that is, the structure of local search neighboring plays a crucial role in establishing an effective LS.  For MkDP, the LS utilized is given by Algorithm~\ref{alg:ls}.
  
  
  \begin{algorithm}[!t] 
  	\caption{LocalSearch}\label{alg:ls}
  	\begin{algorithmic}[1]
  		\STATE \textbf{Input}: a feasible solution $s$
  		\STATE \textbf{Output}: a (possibly) improved solution
  		\STATE improved $\gets$ True
  		\STATE $best\_fit \gets fitness(s)$
  		\STATE $best\_v \gets \emptyset$
 
  		\WHILE{\emph{improved}}
  		     \STATE $improved \gets  False$
  		     \FOR{$v \in V \setminus s$}
  		          \STATE $s' \gets s \cup \{v\}$
  		          \IF{$fitness(s') < fitness(s)$}
  		              \STATE $best\_v \gets v$
  		              \STATE $best\_fit \gets fitness(s')$
  		          \ENDIF
  		     \ENDFOR
  		     \IF{\emph{improved}}
  		         \STATE $s \gets s \cup \{best\_v\}$
  		         \STATE $curr\_fit \gets best\_fit$
  		     \ENDIF

  		\ENDWHILE   		    
  		 \STATE  $improved \gets True$
  		 \WHILE{\emph{improved}}
  		   \STATE $improved \gets  False$
  		    \FOR{$v \in s$}
  		       \STATE $s' \gets \setminus \{v\}$
  		        \IF{$fitness(s') < fitness(s)$}
  		             \STATE $best\_v \gets v$
  		             \STATE $best\_fit \gets fitness(s')$
  		       \ENDIF
  		        \IF{\emph{improved}}
  		       \STATE $s \gets s \setminus \{best\_v\}$
  		       \STATE $curr\_fit \gets best\_fit$
  		       \ENDIF
  		       
  		    \ENDFOR
  		\ENDWHILE
  		\STATE return $s$
  	\end{algorithmic}
\end{algorithm}

Our LS is iteratively trying to improve solution $s$ by applying the best improvement strategy. At each step, it examines all vertex out of solution $s$, and chooses the best one (if any) which contributes most to the fitness decrease, further being added to $s$. Afterwards, all options to removing a vertex from $s$ are examined and the best is chosen and   removed from $s$.   The algorithm stops at the first iteration  in which no improvement in the fitness score is being detected.  We mention that the order of vertices we iterate through at Line 8 and Line 23 are randomly shuffled. 

\emph{Partial fitness evaluation}. Calculating fitness function (\ref{eq:fitness}) takes $O(|E|)$ time; its the most time consuming part is the calculation of $\emph{violate}(\cdot)$  score.   If graphs are dense, this complexity may increase to  $O(n^2)$. Note that at each iteration in Algorithm~\ref{alg:ls},  fitness function evaluation is performed $|V|$ times. Thus, performing the \emph{fitness} evaluation here from scratch is too costly. 

In order to perform \texttt{LocalSearch} more efficiently, partial calculations of the \emph{fitness} function are applied. More in details, before we enter the first \texttt{while} loop, $C(s, v), \forall v \in V$, values are pre-determined (cashed) for solution $s$, using a set structure. In case of a node addition, that is  $s' = s \cup \{v\} $, $violate(s') = violate(s) - C(s, v)  \texttt{I}_{(k> C(s,v))}  + \sum_{w\in N(v) \cap s'} \texttt{I}_{(k+1 > C(s, w))}  $ where \texttt{I} stands for the indicator function. 
In case of a node removal, we apply a similar concept as for a node addition, that is, if $s' = s \setminus \{v\}$, then $violate(s') =  violate(s) + ( k - C(s, v)) \texttt{I}_{( k > C(s, v))} $+ $\sum_{ w \in   N(v)  \setminus s} \texttt{I}_{(k+1 \leq C(s, w)) }$. By encoutering a new incumbent in \texttt{LocalSearch}, cache structure $C(s, \cdot)$ has been updated accordingly.


\section{Experimental evaluation}\label{sec:experiments}


In this section we provide empirical evidence of the quality of the proposed \textsc{Vns} method. In order to do so, we include three competitor heuristic approaches from the literature. To me more precise, the following four methods for MkDP are compared:  

\begin{itemize}
	\item Greedy method from~\cite{parekh1991analysis,gagarin2013randomized}, labelled by \textsc{Greedy-1};
	\item Greedy method from~\cite{gagarin2018multiple}, labelled by \textsc{Greedy-2};
	\item Beam search approach from~\cite{corcoran2021heuristics}, labelled by \textsc{Bs};
	\item VNS approach, as described in Section~\ref{sec:vns}, labelled by \textsc{Vns}.  
\end{itemize}


\emph{Benchmark instances}. For the comparison purposes, we consider the set of real--world instances. This set consists of 20 small-to-middle-sized instance problems and five large-sized instance problems, introduced in~\cite{corcoran2021heuristics}. All instances are  modeling street networks by means of network reachability graphs. Five large sized street networks correspond to international cities such as Belgrade, Berlin, Boston, Dublin, and Minsk.
 Characteristics of large-sized instances in terms of number of vertices and edges are given in Table~\ref{tab:big_instances_chars}. \fxnote{TODO: Milan}
 
 \begin{table}
 	\begin{tabular}{lcc}
 		\textbf{City}      & $|V|$ & $|E|$ \\ \hline
 		Belgrade  & XX    &   XX  \\ 
 		Berlin        & XX    &   XX \\
 	    Boston        & XX    &   XX \\
 	    Dublin        & XX    &   XX \\
 	    Minsk        & XX    &   XX \\ \hline
 	\end{tabular}
     \caption{Large-sized instance characteristics.}
          \label{tab:big_instances_chars}  
 \end{table}
 
 Characteristics of large-sized instances can be found in~\cite{corcoran2021heuristics}. For the shake of completeness, the plots providing vertex numbers and edge numbers of small-to-middle-sized instances is given in Figure \fxnote{TODO: Milan} 
 
 \emph{Testing environments and applied methodology}. The experiments were conducted in a single-core on a computer with Intel Core i9-9900KF CPU @3.6GHz with a memory limit of 6GB RAM per execution, under Microsoft Windows 10 Pro OS. Note that \textsc{Vns}  is implemented in Python 3.9. The results of \textsc{Greedy-1}, \textsc{Greedy-2} and \textsc{Bs} are taken from~\cite{corcoran2021heuristics} for small-sized instances\footnote{These instances are obtained from the authors of~\cite{corcoran2021heuristics} in the original shape} as reported in their experimental section. The results of the  five large-sized instances\footnote{These instances have been generated by using the script obtained from the authors of~\cite{corcoran2021heuristics}. As the script uses up-to-date street networks of the five cities, the large-sized instances differ from the instances introduced in the original publication. } are obtained by executing our re-implementation of \textsc{Greedy-1}, \textsc{Greedy-2}, and \textsc{Bs} algorithms. In all cases, \textsc{Greedy-1} and \textsc{Greedy-2} are run once per each problem instance, as they are deterministic. \textsc{Bs} and \textsc{Vns} are run 10 times per each problem instance.  The termination criteria of our \textsc{Vns} are: ($i$) the maximal time of 30~minutes, i.e., 1800 seconds is exceeded; and ($ii$) the maximal number of iteration of 1 million has reached. 
\subsection{Parameters tuning}
   Among the four competitor algorithms, the parameters of \textsc{Vns} are matter of tuning. The following parameters are tuned: $d_{min}, d_{max},$ and  $penalty$. Domain of these parameters used for the tuning purposes are given in Table~\ref{tab:domain_tuning}.
   
    \begin{table}[ht]
   	\begin{tabular}{ll}
   		parameters      & domain \\ \hline
   	$d_{min}$ &  -- \\
   	$d_{max}$ &  -- \\
   	 $penalty$ & --  \\   \hline
   	\end{tabular}
   	\caption{Parameter domains of \textsc{Vns}.}  
   	   	\label{tab:domain_tuning}
   \end{table}
   
   Concerning tuning tools, two different tools are chosen: ($i$) a greedy--based tool xx and ($ii$) evolutionary--based tool \texttt{PyGAD}. The both tools may be directly included in Python by importing the respective packages.
   \fxnote{Milan: dio oko podesavanja tuninga -- koje instance, koliki je budet broja pustanja, isl.}
   
    \texttt{GridSearch} tool has returned the following configuration setting: TODO
    
     \texttt{PyGAD} tool has returned the following configuration setting: TODO
\subsection{Numerical results }
 TODO: Marko
\section{Conclusions and future work}
 TODO
 
\section*{Acknowledgments} 
TODO
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{gecco_poster_literature}

%%
%% If your work has an appendix, this is the place to put it.
%\appendix
 
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
